{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LongPegasus.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMH/O9PSaY7dGTrq9R06Bw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhilash1910/LongPegasus/blob/master/LongPegasus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRKua4rZLGIV"
      },
      "source": [
        "## LongPegasus\n",
        "\n",
        "[LongPegasus package](https://pypi.org/project/LongPegasus/) is used for inducing longformer self attention over base pegasus abstractive summarization model to increase the token limit and performance.The Pegasus is a large Transformer-based encoder-decoder model with a new pre-training objective which is adapted to abstractive summarization. More specifically, the pre-training objective, called \"Gap Sentence Generation (GSG)\", consists of masking important sentences from a document and generating these gap-sentences.On the other hand, the Longformer is a Transformer which replaces the full-attention mechanism (quadratic dependency) with a novel attention mechanism which scale linearly with the input sequence length. Consequently, Longformer can process sequences up to 4,096 tokens long (8 times longer than BERT which is limited to 512 tokens).This package plugs Longformers attention mechanism to Pegasus in order to perform abstractive summarization on long documents. The base modules are built on Tensorflow platform.\n",
        "\n",
        "<img src=\"https://camo.githubusercontent.com/ce02c86985f8a13553cfac760b4a000fa00c328a26c89c6da2ccc55f7a9a6367/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313138342f312a797035784c61564c37764f733659543951504f324f772e706e67\">\n",
        "\n",
        "\n",
        "### Usage \n",
        "\n",
        "This notebook provides a demo for the package (0.3) and can be followed from the [repository](https://github.com/abhilash1910/LongPegasus/blob/master/driver_test_long_model.py)\n",
        "The package (stable release 0.3) can be installed from Pypi using the following code:\n",
        "\n",
        "!pip install LongPegasus==0.3\n",
        "\n",
        "Rest of the details for using the library are mentioned in [readme](https://github.com/abhilash1910/LongPegasus#usage)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyJk9NesrA5L",
        "outputId": "b88034b1-097d-4c87-b910-5b2a899791d5"
      },
      "source": [
        "!pip install LongPegasus"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting LongPegasus\n",
            "  Downloading LongPegasus-0.3.tar.gz (3.7 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from LongPegasus) (2.7.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from LongPegasus) (4.12.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from LongPegasus) (0.1.96)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (12.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.19.5)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (2.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (0.22.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (3.10.0.2)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.42.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (0.37.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->LongPegasus) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->LongPegasus) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->LongPegasus) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->LongPegasus) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->LongPegasus) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->LongPegasus) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->LongPegasus) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->LongPegasus) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->LongPegasus) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->LongPegasus) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->LongPegasus) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->LongPegasus) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->LongPegasus) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->LongPegasus) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->LongPegasus) (3.1.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (3.4.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->LongPegasus) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->LongPegasus) (3.0.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->LongPegasus) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->LongPegasus) (7.1.2)\n",
            "Building wheels for collected packages: LongPegasus\n",
            "  Building wheel for LongPegasus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for LongPegasus: filename=LongPegasus-0.3-py3-none-any.whl size=4525 sha256=4b97825fbbb005f5e11700a86277bfd69d6a1f7774ec2a5cab795b107c46cae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/1c/74/f10292a3cfab6e1c51c338a76e23becba1139191dddfcc0260\n",
            "Successfully built LongPegasus\n",
            "Installing collected packages: LongPegasus\n",
            "Successfully installed LongPegasus-0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJG4-SjTGcEN",
        "outputId": "2ff5faa6-9df9-458f-e06b-bfc098339a73"
      },
      "source": [
        "from LongPegasus.LongPegasus import LongPegasus\n",
        "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
        "\n",
        "if __name__=='__main__':\n",
        "    l=LongPegasus()             \n",
        "    model_name='human-centered-summarization/financial-summarization-pegasus'            \n",
        "    model,tokenizer=l.create_long_model(save_model=\"Pegasus\\\\\", attention_window=4096, max_pos=4096,model_name=model_name)\n",
        "    model = TFPegasusForConditionalGeneration.from_pretrained('Pegasus\\\\')\n",
        "    tokenizer = PegasusTokenizer.from_pretrained('Pegasus\\\\')\n",
        "\n",
        "    ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        "    )\n",
        "    inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=4096, return_tensors='tf')\n",
        "    \n",
        "    # Generate Summary\n",
        "    summary_ids = model.generate(inputs['input_ids'])\n",
        "    print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFPegasusForConditionalGeneration.\n",
            "\n",
            "All the layers of TFPegasusForConditionalGeneration were initialized from the model checkpoint at human-centered-summarization/financial-summarization-pegasus.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFPegasusForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_encoder_position_embeddings:  4096\n",
            "max_decoder_position_embeddings:  512\n",
            "=======Model Configuration=======\n",
            "LongformerPegasusConfig {\n",
            "  \"_name_or_path\": \"google/pegasus-xsum\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"architectures\": [\n",
            "    \"LongformerForPegasus\"\n",
            "  ],\n",
            "  \"attention_dilation\": [\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"attention_mode\": \"sliding_chunks\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": [\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096,\n",
            "    4096\n",
            "  ],\n",
            "  \"autoregressive\": false,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 16,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"do_blenderbot_90_layernorm\": false,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 16,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"extra_pos_embeddings\": 1,\n",
            "  \"force_bos_token_to_be_generated\": false,\n",
            "  \"forced_eos_token_id\": 1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_dropout_prob\": 1e-05,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"initializer_range\": null,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 0.6,\n",
            "  \"max_decoder_position_embeddings\": 512,\n",
            "  \"max_encoder_position_embeddings\": 4096,\n",
            "  \"max_length\": 64,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"pegasus\",\n",
            "  \"normalize_before\": true,\n",
            "  \"normalize_embedding\": false,\n",
            "  \"num_beams\": 8,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"scale_embedding\": true,\n",
            "  \"static_position_embeddings\": true,\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 96103\n",
            "}\n",
            "\n",
            "==============Model & Tokenizer saved===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at Pegasus\\ were not used when initializing TFPegasusForConditionalGeneration: ['']\n",
            "- This IS expected if you are initializing TFPegasusForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFPegasusForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFPegasusForConditionalGeneration were not initialized from the model checkpoint at Pegasus\\ and are newly initialized: ['model/encoder/layers.13/self_attn/v_proj/kernel:0', 'model/encoder/layers.10/self_attn/v_proj/bias:0', 'model/encoder/layers.3/self_attn/out_proj/bias:0', 'model/encoder/layers.6/self_attn/v_proj/bias:0', 'model/encoder/layers.8/self_attn/k_proj/kernel:0', 'model/encoder/layers.4/self_attn/out_proj/bias:0', 'model/encoder/layers.1/self_attn/q_proj/kernel:0', 'model/encoder/layers.10/self_attn/k_proj/bias:0', 'model/encoder/layers.11/self_attn/q_proj/kernel:0', 'model/encoder/layers.6/self_attn/k_proj/bias:0', 'model/encoder/layers.7/self_attn/q_proj/bias:0', 'model/encoder/layers.11/self_attn/q_proj/bias:0', 'model/encoder/layers.6/self_attn/out_proj/kernel:0', 'model/encoder/layers.8/self_attn/q_proj/bias:0', 'model/encoder/layers.3/self_attn/v_proj/kernel:0', 'model/encoder/layers.8/self_attn/q_proj/kernel:0', 'model/encoder/layers.10/self_attn/q_proj/kernel:0', 'model/encoder/layers.9/self_attn/k_proj/kernel:0', 'model/encoder/layers.9/self_attn/v_proj/bias:0', 'model/encoder/layers.4/self_attn/out_proj/kernel:0', 'model/encoder/layers.13/self_attn/v_proj/bias:0', 'model/encoder/layers.11/self_attn/v_proj/bias:0', 'model/encoder/layers.9/self_attn/v_proj/kernel:0', 'model/encoder/layers.14/self_attn/v_proj/kernel:0', 'model/encoder/layers.1/self_attn/q_proj/bias:0', 'model/encoder/layers.2/self_attn/v_proj/kernel:0', 'model/encoder/layers.6/self_attn/v_proj/kernel:0', 'model/encoder/layers.12/self_attn/out_proj/bias:0', 'model/encoder/layers.13/self_attn/q_proj/bias:0', 'model/encoder/layers.7/self_attn/k_proj/kernel:0', 'model/encoder/layers.13/self_attn/out_proj/bias:0', 'model/encoder/layers.14/self_attn/out_proj/kernel:0', 'model/encoder/layers.0/self_attn/k_proj/bias:0', 'model/encoder/layers.14/self_attn/k_proj/bias:0', 'model/encoder/layers.9/self_attn/q_proj/bias:0', 'model/encoder/layers.12/self_attn/k_proj/kernel:0', 'model/encoder/layers.9/self_attn/out_proj/bias:0', 'model/encoder/layers.0/self_attn/q_proj/kernel:0', 'model/encoder/layers.12/self_attn/out_proj/kernel:0', 'model/encoder/layers.4/self_attn/v_proj/bias:0', 'model/encoder/layers.6/self_attn/q_proj/kernel:0', 'model/encoder/layers.3/self_attn/k_proj/kernel:0', 'model/encoder/layers.4/self_attn/v_proj/kernel:0', 'model/encoder/layers.11/self_attn/out_proj/bias:0', 'model/encoder/layers.9/self_attn/k_proj/bias:0', 'model/encoder/layers.1/self_attn/v_proj/kernel:0', 'model/encoder/layers.2/self_attn/q_proj/kernel:0', 'model/encoder/layers.9/self_attn/q_proj/kernel:0', 'model/encoder/layers.15/self_attn/k_proj/bias:0', 'model/encoder/layers.12/self_attn/k_proj/bias:0', 'model/encoder/layers.10/self_attn/k_proj/kernel:0', 'model/encoder/layers.2/self_attn/v_proj/bias:0', 'model/encoder/layers.5/self_attn/out_proj/kernel:0', 'model/encoder/layers.13/self_attn/k_proj/bias:0', 'model/encoder/layers.12/self_attn/v_proj/bias:0', 'model/encoder/layers.0/self_attn/out_proj/bias:0', 'model/encoder/layers.1/self_attn/k_proj/kernel:0', 'model/encoder/layers.7/self_attn/out_proj/bias:0', 'model/encoder/layers.7/self_attn/q_proj/kernel:0', 'model/encoder/layers.5/self_attn/k_proj/kernel:0', 'model/encoder/embed_positions/embeddings:0', 'model/encoder/layers.1/self_attn/out_proj/bias:0', 'model/encoder/layers.5/self_attn/k_proj/bias:0', 'model/encoder/layers.3/self_attn/k_proj/bias:0', 'model/encoder/layers.10/self_attn/out_proj/bias:0', 'model/encoder/layers.7/self_attn/out_proj/kernel:0', 'model/encoder/layers.8/self_attn/v_proj/bias:0', 'model/encoder/layers.10/self_attn/q_proj/bias:0', 'model/encoder/layers.15/self_attn/out_proj/kernel:0', 'model/encoder/layers.0/self_attn/k_proj/kernel:0', 'model/encoder/layers.4/self_attn/q_proj/bias:0', 'model/encoder/layers.11/self_attn/k_proj/kernel:0', 'model/encoder/layers.12/self_attn/q_proj/kernel:0', 'model/encoder/layers.8/self_attn/out_proj/bias:0', 'model/encoder/layers.5/self_attn/v_proj/kernel:0', 'model/encoder/layers.3/self_attn/q_proj/bias:0', 'model/encoder/layers.14/self_attn/q_proj/kernel:0', 'model/encoder/layers.14/self_attn/k_proj/kernel:0', 'model/encoder/layers.14/self_attn/out_proj/bias:0', 'model/encoder/layers.2/self_attn/k_proj/bias:0', 'model/encoder/layers.5/self_attn/q_proj/kernel:0', 'model/encoder/layers.8/self_attn/k_proj/bias:0', 'model/encoder/layers.14/self_attn/q_proj/bias:0', 'model/encoder/layers.15/self_attn/out_proj/bias:0', 'model/encoder/layers.15/self_attn/q_proj/bias:0', 'model/encoder/layers.1/self_attn/v_proj/bias:0', 'model/encoder/layers.0/self_attn/q_proj/bias:0', 'model/encoder/layers.5/self_attn/out_proj/bias:0', 'model/encoder/layers.4/self_attn/q_proj/kernel:0', 'model/encoder/layers.2/self_attn/out_proj/kernel:0', 'model/encoder/layers.13/self_attn/k_proj/kernel:0', 'model/encoder/layers.9/self_attn/out_proj/kernel:0', 'model/encoder/layers.7/self_attn/v_proj/bias:0', 'model/encoder/layers.15/self_attn/v_proj/bias:0', 'model/encoder/layers.0/self_attn/out_proj/kernel:0', 'model/encoder/layers.1/self_attn/out_proj/kernel:0', 'model/encoder/layers.7/self_attn/v_proj/kernel:0', 'model/encoder/layers.2/self_attn/out_proj/bias:0', 'model/encoder/layers.15/self_attn/k_proj/kernel:0', 'model/encoder/layers.4/self_attn/k_proj/bias:0', 'model/encoder/layers.4/self_attn/k_proj/kernel:0', 'model/encoder/layers.0/self_attn/v_proj/kernel:0', 'model/encoder/layers.13/self_attn/out_proj/kernel:0', 'model/encoder/layers.2/self_attn/k_proj/kernel:0', 'model/encoder/layers.2/self_attn/q_proj/bias:0', 'model/encoder/layers.13/self_attn/q_proj/kernel:0', 'model/encoder/layers.11/self_attn/out_proj/kernel:0', 'model/encoder/layers.1/self_attn/k_proj/bias:0', 'model/encoder/layers.11/self_attn/v_proj/kernel:0', 'model/encoder/layers.10/self_attn/out_proj/kernel:0', 'model/encoder/layers.7/self_attn/k_proj/bias:0', 'model/encoder/layers.15/self_attn/q_proj/kernel:0', 'model/encoder/layers.8/self_attn/out_proj/kernel:0', 'model/encoder/layers.8/self_attn/v_proj/kernel:0', 'model/encoder/layers.5/self_attn/v_proj/bias:0', 'model/encoder/layers.15/self_attn/v_proj/kernel:0', 'model/encoder/layers.5/self_attn/q_proj/bias:0', 'model/encoder/layers.11/self_attn/k_proj/bias:0', 'model/encoder/layers.3/self_attn/out_proj/kernel:0', 'model/encoder/layers.3/self_attn/q_proj/kernel:0', 'model/encoder/layers.3/self_attn/v_proj/bias:0', 'model/encoder/layers.6/self_attn/q_proj/bias:0', 'model/encoder/layers.0/self_attn/v_proj/bias:0', 'model/encoder/layers.6/self_attn/k_proj/kernel:0', 'model/encoder/layers.12/self_attn/q_proj/bias:0', 'model/encoder/layers.6/self_attn/out_proj/bias:0', 'model/encoder/layers.14/self_attn/v_proj/bias:0', 'model/encoder/layers.12/self_attn/v_proj/kernel:0', 'model/encoder/layers.10/self_attn/v_proj/kernel:0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thousand people have been affected by the wildfires.']\n"
          ]
        }
      ]
    }
  ]
}